{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","file_extension":".py","version":"3.5.4","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"}},"name":"lesson4_part2_Decision_trees.ipynb","hide_input":false,"nbTranslate":{"hotkey":"alt-t","sourceLang":"en","targetLang":"fr","displayLangs":["*"],"langInMainMenu":true,"useGoogleTranslate":true},"toc":{"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"base_numbering":1,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"lang":"fr","id":"QGSFji5V_bkf"},"source":["*La base des TPs pour le cours \"Classification des données\" a été prise du cours en ligne \"Open Machine Learning Course\" (https://mlcourse.ai/, auteur Yury Kashnitsky)*\n","\n"]},{"cell_type":"markdown","metadata":{"lang":"fr","id":"DunkPsOn_bkg"},"source":["# <center> TP 2 : Les methodes de régression et classification"]},{"cell_type":"markdown","metadata":{"lang":"fr","id":"BlYPZ1t-_bkg"},"source":["Dans ce TP, vous allez découvrir le fonctionnement d'un arbre de décision, aussi que de la méthode KNN etc., dans les tâches de régression et classification sur les données synthétiques et MNIST.\n","\n","**Votre travail consiste à écrire du code et effectuer des calculs dans les cellules ci-dessous.**"]},{"cell_type":"markdown","metadata":{"id":"QF66Qhb0PSMn"},"source":["Tout d'abord, nous allons initialiser l'environnement, importons tout les bibliothèques nécessaires"]},{"cell_type":"code","metadata":{"trusted":false,"id":"8u6cpJvU_bkh"},"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns \n","sns.set()\n","\n","from IPython.display import Image\n","from matplotlib import pyplot as plt\n","from matplotlib import rcParams\n","rcParams['figure.figsize'] = 11, 8\n","%config InlineBackend.figure_format = 'retina'\n","\n","from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n","from sklearn.metrics import accuracy_score\n","from sklearn.tree import export_graphviz\n","from sklearn.preprocessing import StandardScaler"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"lang":"fr","id":"ZdnwXCto_bkl"},"source":["## 1. Arbres de décision et KNN methode pour la régression : données synthétiques"]},{"cell_type":"markdown","metadata":{"lang":"fr","id":"WZFrHdSW_bkl"},"source":["Considérons le problème de régression unidimensionnel suivant. Nous devons créer une fonction $\\large a(x)$ pour approximer la dépendance $\\large y = f(x) = x^3 $ en utilisant le critère d'erreur quadratique moyen: $\\large \\min \\sum_i {(a(x_i) - f(x_i))}^2$."]},{"cell_type":"markdown","metadata":{"id":"lafz-yApVAEE"},"source":["Créons une fonction de génération des données (sur l'intervalle [-3,3]) avec le bruit normal de 40% pour ce problème."]},{"cell_type":"code","metadata":{"id":"YqPslyskXgeD"},"source":["n_train = 100        \n","n_test = 100     \n","noise = 0.4\n","\n","def generate(n_samples, noise):\n","  X = np.linspace(-3,3,n_samples)\n","  y = X ** 3 + np.random.normal(0.0, noise, n_samples) \n","  X = X.reshape((n_samples, 1))\n","  \n","  return X, y\n","\n","X_train, y_train = generate(n_samples=n_train, noise=noise)\n","X_test, y_test = generate(n_samples=n_test, noise=noise)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TnOzLuZjTajj"},"source":["**Ex. 1** : Créez un arbre de decision de la classe [`sklearn.tree.DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html), entraînez le sur données {X_train, y_train} et visualisez les prédictions qu’il réalise. Affichez votre l'erreur quadratique moyenne (MSE) obtenue sur les données de test {X_test, y_test}.\n"]},{"cell_type":"code","metadata":{"id":"-kY89--nXiS9"},"source":["# You code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sGDWiwSoXuzG"},"source":["**Ex. 2** : Appliquez la méthode des plus proches voisins (KNN) de la classe [`sklearn.neighbors.KNeighborsRegressor`]( https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) à ce problème de régression, entraînez le sur données {X_train, y_train} et visualisez les prédictions qu’il réalise. Affichez votre l'erreur quadratique moyenne (MSE) obtenue sur les données de test {X_test, y_test}."]},{"cell_type":"code","metadata":{"trusted":false,"id":"kM-OLYIr_bkz"},"source":["# You code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Arbres de décision, k-NN et Forêt aléatoire dans une tâche de reconnaissance manuscrite de chiffres du MNIST"],"metadata":{"id":"v0cf2KtsS9hQ"}},{"cell_type":"markdown","source":["Voyons maintenant comment ces 3 algorithmes fonctionnent sur une tâche tirée du monde réel. Nous allons utiliser le jeu de données intégré dans `sklearn` sur des chiffres manuscrits. Cette tâche est un exemple où k-NN fonctionne étonnamment bien.\n"," \n","Les images sont des matrices 8x8 (intensité de la couleur blanche pour chaque pixel). Ensuite, chacune de ces matrices est \"unfolded\" (étalée, dépliée) en un vecteur de longueur 64 et nous obtenons une description des caractéristiques d'un objet.\n"," \n","Affichons quelques chiffres manuscrits. Nous voyons que l'on peut les distinguer."],"metadata":{"id":"DGAmEwbBTMha"}},{"cell_type":"code","source":["from sklearn.datasets import load_digits\n","\n","data = load_digits()\n","X, y = data.data, data.target\n","\n","X[0,:].reshape([8,8])"],"metadata":{"id":"OLvCYyLJS49H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f, axes = plt.subplots(1, 4, sharey=True, figsize=(16,6))\n","for i in range(4):\n","    axes[i].imshow(X[i,:].reshape([8,8]), cmap='Greys');"],"metadata":{"id":"FddHFSLWTSlW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Ex. 3** : Séparez les données MNIST en *train* (70%) et *test* (30%) sous-ensemble des données (en utilisant [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html))"],"metadata":{"id":"VDa-K31tT8Yp"}},{"cell_type":"code","source":["# You code here"],"metadata":{"id":"o7c2SkDlUUoQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Ex. 4,5,6** : Appliquez les Arbres de décision ([`sklearn.tree.DecisionTreeClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)), k-NN ([`sklearn.neighbors.KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)) et Forêt aléatoire ([`sklearn.ensemble.RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)) avec les paramètres par défaut sur les données MNIST séparées. \n","\n","Affichez les résultats obtenus sur les données de *test*."],"metadata":{"id":"tXPUiooET8Mz"}},{"cell_type":"code","source":["# You code here"],"metadata":{"id":"OSfOer-aVp2A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Ex. 7,8,9** : Optimisez les paramètres des 3 méthodes en utilisant la validation croisée (sur les données de *train*), [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). \n","\n","Affichez les résultats obtenus sur les données de *test*."],"metadata":{"id":"kgTzaVWkVvTB"}},{"cell_type":"code","source":["# You code here"],"metadata":{"id":"jCr8A9vTVuJk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Ex. 10** : Organisez les résultats obtenus dans les ex.8-9 sous forme d'un tableau (lignes : DT, KNN, RF; colonnes : Holdout, CV)."],"metadata":{"id":"eV2-3RA-XFM0"}},{"cell_type":"code","source":["# You code here"],"metadata":{"id":"2CN-HeveXFf3"},"execution_count":null,"outputs":[]}]}